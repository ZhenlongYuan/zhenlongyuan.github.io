---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<span class='anchor' id='about-me'></span>

# ğŸ‘‹ About Me 
I am currently pursuing my Ph.D. at the Institute of Computing Technology <img src='./images/ICT.png' style="width: 1.9em;">, Chinese Academy of Sciences <img src='./images/UCAS.png' style="width: 1.6em;">, advised by Prof. [Zhaoqi Wang](http://english.ict.cas.cn/people/scien/bln/202303/t20230315_328238.html). Concurrently, I serve as a Research Intern at AMAP <img src='./images/AMAP.png' style="width: 1.8em;">, Alibaba <img src='./images/Alibaba.png' style="width: 1.7em;">, where I work closely with [Xiangxiang Chu](https://cxxgtxy.github.io/). I am deeply grateful for the opportunity to collaborate with exceptional researchers including Prof. [Shuo Li](https://scholar.google.com/citations?user=6WNtJa0AAAAJ&hl=en), Prof. [Yujun Cai](https://vanoracai.github.io/), and Prof. [Yiwei Wang](https://wangywust.github.io/). Their mentorship and insights have profoundly shaped my academic journey.

My research interest includes Vision-Language Model (VLM), Large Language Model (LLM), Embodied Agents, Multimodal AI, and 3D Vision. I have published 20+ papers at the top international AI conferences such as NeurIPS, ICLR, ICML, CVPR, ICCV, AAAI, etc.

# ğŸ“š Research Interests
- ***Foundation Models & Pre-training ğŸ”¥ğŸ”¥***
  - Vision-Language Models (VLMs) / Vision-Language Action (VLA) / Spatial Intelligence
- ***Model Enhancement & Post-training ğŸ”¥ğŸ”¥***
  - Reasoning & Alignment / Tool-Augmented RL / NLP-Enhanced Training
- ***Model Interpretation ğŸ”¥ğŸ”¥***
  - Mechanistic Interpretability / Factuality, Truthfulness, and Social Good
- ***Real-World ApplicationsğŸ”¥ğŸ”¥***
  - Embodied Agents / AI for Science / Biomedical Engineering


# ğŸ”¥ Main News

- *2025.11*: &nbsp;ğŸ‰ğŸ‰ We propose [Reasoning-VLA](https://arxiv.org/abs/2511.19912), which is now available on ArXiv!
- *2025.10*: &nbsp;ğŸ‰ğŸ‰ Our work [DVP-MVS++](https://arxiv.org/abs/2506.13215) has been ***Accepted*** by ***TCSVT 2025***.
- *2025.10*: &nbsp;ğŸ‰ğŸ‰ We propose [Video-STAR](https://arxiv.org/abs/2510.08480), which is now available on ArXiv!
- *2025.08*: &nbsp;ğŸ‰ğŸ‰ Our work [AutoDrive-RÂ²](https://arxiv.org/abs/2509.01944v1) was reported by [AutoDrive Heart (è‡ªåŠ¨é©¾é©¶ä¹‹å¿ƒ)](https://mp.weixin.qq.com/s/7y0-CMAkls16iumNK3mlXg)
- *2025.08*: &nbsp;ğŸ‰ğŸ‰ We propose [AutoDrive-RÂ²](https://arxiv.org/abs/2509.01944v1), which is now available on ArXiv!
- *2025.06*: &nbsp;ğŸ‰ğŸ‰ We propose [DVP-MVS++](https://arxiv.org/abs/2506.13215), which is now available on ArXiv!
- *2025.05*: &nbsp;ğŸ‰ğŸ‰ Our work [SED-MVS](https://ieeexplore.ieee.org/document/11016951) has been ***Accepted*** by ***TCSVT 2025***.
- *2024.12*: &nbsp;ğŸ‰ğŸ‰ We propose [SED-MVS](https://arxiv.org/abs/2503.13721), which is now available on ArXiv!
- *2024.12*: &nbsp;ğŸ‰ğŸ‰ Our work [DVP-MVS](https://ojs.aaai.org/index.php/AAAI/article/view/33056) has been ***Accepted*** by ***AAAI 2025***.
- *2024.12*: &nbsp;ğŸ‰ğŸ‰ Our work [MSP-MVS](https://ojs.aaai.org/index.php/AAAI/article/view/33057) has been ***Accepted*** by ***AAAI 2025***.
- *2024.08*: &nbsp;ğŸ‰ğŸ‰ We propose [DVP-MVS](https://arxiv.org/abs/2412.11578), which is now available on ArXiv!
- *2024.08*: &nbsp;ğŸ‰ğŸ‰ We propose [MSP-MVS](https://arxiv.org/abs/2407.19323), which is now available on ArXiv!
- *2024.05*: &nbsp;ğŸ‰ğŸ‰ Our work [TSAR-MVS](https://www.sciencedirect.com/science/article/pii/S0031320324003169) has been ***Accepted*** by ***PR 2024***.
- *2024.01*: &nbsp;ğŸ‰ğŸ‰ We propose [TSAR-MVS](https://arxiv.org/abs/2308.09990), which is now available on ArXiv!
- *2023.12*: &nbsp;ğŸ‰ğŸ‰ Our work [SD-MVS](https://ojs.aaai.org/index.php/AAAI/article/view/28512) has been ***Accepted*** by ***AAAI 2024***.
- *2023.09*: &nbsp;ğŸ‰ğŸ‰ We propose [SD-MVS](https://arxiv.org/abs/2401.06385), which is now available on ArXiv!

# ğŸ“ Main Publications 

## Multimodal LLMs Post-Training

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Preprint</div><img src='images/VideoSTAR.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Video-STAR: Reinforcing Zero-shot Video Understanding with Tools](https://arxiv.org/abs/2510.08480)

<span style="display: inline-block; padding: 2px 10px; margin: 2px 4px 2px 0; background-color: #e8f4f8; color: #0077b6; border-radius: 15px; font-size: 12px;">Think with Videos</span>
<span style="display: inline-block; padding: 2px 10px; margin: 2px 4px 2px 0; background-color: #fff3e0; color: #e65100; border-radius: 15px; font-size: 12px;">Tool-Using Agent</span>
<span style="display: inline-block; padding: 2px 10px; margin: 2px 4px 2px 0; background-color: #f3e5f5; color: #7b1fa2; border-radius: 15px; font-size: 12px;">Multi-turn Agentic RL</span>

**Yuan, Z.**, Qu X., Qian, C., Chen, R., Tang, J., Sun L., Chu X., Zhang D., Wang Y., Cai Y., Li S.

[[Paper]](https://arxiv.org/abs/2510.08480)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Preprint</div><img src='images/AutoDrive-R2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[AutoDrive-RÂ²: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving](https://arxiv.org/abs/2509.01944v1)

<span style="display: inline-block; padding: 2px 10px; margin: 2px 4px 2px 0; background-color: #e8f4f8; color: #0077b6; border-radius: 15px; font-size: 12px;">Multimodal Reasoning</span>
<span style="display: inline-block; padding: 2px 10px; margin: 2px 4px 2px 0; background-color: #fff3e0; color: #e65100; border-radius: 15px; font-size: 12px;">Autonomous Driving</span>
<span style="display: inline-block; padding: 2px 10px; margin: 2px 4px 2px 0; background-color: #f3e5f5; color: #7b1fa2; border-radius: 15px; font-size: 12px;">Open-World Applications</span>

**Featured by [AutoDrive Heart (è‡ªåŠ¨é©¾é©¶ä¹‹å¿ƒ)](https://mp.weixin.qq.com/s/7y0-CMAkls16iumNK3mlXg)**

**Yuan, Z.**, Tang, J., Luo, J., Chen, R., Qian, C., Sun, L., Cai Y., Zhang D., Li, S

[[Paper]](https://arxiv.org/abs/2509.01944v1)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Preprint</div><img src='images/Reasoning-VLA.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Reasoning-VLA: A Fast and General Vision-Language-Action Reasoning Model for Autonomous Driving](https://arxiv.org/abs/2511.19912)

<span style="display: inline-block; padding: 2px 10px; margin: 2px 4px 2px 0; background-color: #e8f4f8; color: #0077b6; border-radius: 15px; font-size: 12px;">Multimodal Reasoning</span>
<span style="display: inline-block; padding: 2px 10px; margin: 2px 4px 2px 0; background-color: #fff3e0; color: #e65100; border-radius: 15px; font-size: 12px;">Autonomous Driving</span>
<span style="display: inline-block; padding: 2px 10px; margin: 2px 4px 2px 0; background-color: #f3e5f5; color: #7b1fa2; border-radius: 15px; font-size: 12px;">Open-World Applications</span>

Zhang D.*, <span style="font-weight: bold;">Yuan, Z.*</span>, Chen Z., Liao C., Chen Y., Shen F., Zhou Q., Chua T.

[[Paper]](https://arxiv.org/abs/2511.19912)
</div>
</div>

## Generative Foundation Model

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Preprint</div><img src='images/Q1.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
From Scale to Speed: Adaptive Test-Time Scaling for Image Editing

<span style="display: inline-block; padding: 2px 10px; margin: 2px 4px 2px 0; background-color: #e8f4f8; color: #0077b6; border-radius: 15px; font-size: 12px;">Generation Model</span>
<span style="display: inline-block; padding: 2px 10px; margin: 2px 4px 2px 0; background-color: #fff3e0; color: #e65100; border-radius: 15px; font-size: 12px;">Image Editing</span>
<span style="display: inline-block; padding: 2px 10px; margin: 2px 4px 2px 0; background-color: #f3e5f5; color: #7b1fa2; border-radius: 15px; font-size: 12px;">Text-to-Image Generation</span>

Qu X.*, <span style="font-weight: bold;">Yuan, Z.*</span>, Tang J., Chen R., Tang D., Yu M., Sun L., Bai Y., Chu X., Gou G,., Xiong G., Cai Y.

</div>
</div>

## 3D Vision

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TCSVT</div><img src='images/DVP-MVS++.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[DVP-MVS++: Synergize Depth-Normal-Edge and Harmonized Visibility Prior for Multi-View Stereo](https://arxiv.org/abs/2506.13215)

**Yuan, Z.**, Zhang, D., Li, Z., Qian, C., Chen, J., Chen, Y., Chen K., Mao T., Li Z, Jiang H., Wang, Z

IEEE Transactions on Circuits and Systems for Video Technology (**IEEE TCSVT**) (Under Review), 2025.

[[Paper]](https://arxiv.org/abs/2506.13215)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TCSVT</div><img src='images/SED-MVS.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SED-MVS: Segmentation-Driven and Edge-Aligned Deformation Multi-View Stereo with Depth Restoration and Occlusion Constraint](https://ieeexplore.ieee.org/document/11016951)

**Yuan, Z**., Yang, Z., Cai, Y., Wu, K., Liu, M., Zhang, D., Jiang H, Li Z., Wang, Z.

IEEE Transactions on Circuits and Systems for Video Technology (**IEEE TCSVT**), 2025.

[[Paper]](https://ieeexplore.ieee.org/document/11016951)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI</div><img src='images/DVP-MVS.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[DVP-MVS: Synergize Depth-Edge and Visibility Prior for Multi-View Stereo](https://ojs.aaai.org/index.php/AAAI/article/view/33056)

**Yuan, Z**., Luo, J., Shen, F., Li, Z., Liu, C., Mao, T., Wang, Z.

AAAI Conference on Artificial Intelligence (**AAAI**), 2025.

[[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/33056)
[[Code]](https://github.com/ZhenlongYuan/DVP-MVS)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI</div><img src='images/MSP-MVS2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[MSP-MVS: Multi-granularity segmentation prior guided multi-view stereo](https://ojs.aaai.org/index.php/AAAI/article/view/33057)

**Yuan, Z.**, Liu, C., Shen, F., Li, Z., Luo, J., Mao, T., Wang, Z.

AAAI Conference on Artificial Intelligence (**AAAI**), 2025.

[[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/33057)
[[Code]](https://github.com/ZhenlongYuan/MSP-MVS)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI</div><img src='images/SD-MVS.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[SD-MVS: Segmentation-driven deformation multi-view stereo with spherical refinement and em optimization](https://ojs.aaai.org/index.php/AAAI/article/view/28512)

**Yuan, Z.**, Cao, J., Li, Z., Jiang, H., Wang, Z.

AAAI Conference on Artificial Intelligence (**AAAI**), 2024.

[[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/28512)
[[Code]](https://github.com/ZhenlongYuan/SD-MVS)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">PR</div><img src='images/TSAR-MVS.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[TSAR-MVS: Textureless-aware segmentation and correlative refinement guided multi-view stereo](https://www.sciencedirect.com/science/article/pii/S0031320324003169)

**Yuan, Z.**, Cao, J., Wang, Z., Li, Z..

Pattern Recognition (**PR**), 2024.

[[Paper]](https://www.sciencedirect.com/science/article/pii/S0031320324003169)
[[Code]](https://github.com/ZhenlongYuan/TSAR-MVS)
</div>
</div>

# ğŸ“– All Publications
- ``Preprint`` [Video-STAR: Reinforcing Zero-shot Video Understanding with Tools.](https://arxiv.org/abs/2510.08480) **Z Yuan**, X Qu, C Qian, et al.
- ``Preprint`` [AutoDrive-R2: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving.](https://arxiv.org/pdf/2509.01944) **Z Yuan**, J Tang, J Luo, et al.
- ``Preprint`` [Pure Vision Language Action (VLA) Models: A Comprehensive Survey.](https://arxiv.org/pdf/2509.19012) D Zhang, J Sun, C Hu, X Wu, **Z Yuan**, et al.
- ``Preprint`` [AT-Drive: Exploiting Adversarial Transfer for End-to-end Autonomous Driving.]() D Zhang, **Z Yuan**, K Huang, et al.
- ``Preprint`` [ADDI: A Simplified E2E Autonomous Driving Model with Distinct Experts and Implicit Interactions.]() D Zhang, **Z Yuan**, Chen Y., et al.
- ``Preprint`` [EMPOWER: Evolutionary Medical Prompt Optimization With Reinforcement Learning.](https://arxiv.org/pdf/2508.17703) Y Chen, Y He, J Yang, D Zhang, **Z Yuan**, et al. 
- ``Preprint`` [DVP-MVS++: Synergize Depth-Normal-Edge and Harmonized Visibility Prior for Multi-View Stereo.](https://arxiv.org/pdf/2506.13215) **Z Yuan**, D Zhang, Z Li, et al.
- ``NIPS 2025`` [InstructHOI: Context-Aware Instruction for Multi-Modal Reasoning in Human-Object Interaction Detection.]() J Luo, W Ren , Q Zheng, Y Zhang, **Z Yuan**, et al.
- ``IEEE TCSVT 2025`` [Learning multi-view stereo with geometry-aware prior.](https://ieeexplore.ieee.org/abstract/document/11029471) K Chen, **Z Yuan**, H Xiao, T Mao, et al. 
- ``HCII 2025`` [MR-IntelliAssist: A World Cognition Agent Enabling Adaptive Human-AI Symbiosis in Industry 4.0.](https://link.springer.com/chapter/10.1007/978-3-031-93429-2_11), C Liu, **Z Yuan**, Y Wang, Y Yin, et al. 
- ``IEEE TCSVT 2025`` [SED-MVS: Segmentation-Driven and Edge-Aligned Deformation Multi-View Stereo with Depth Restoration and Occlusion Constraint.](https://arxiv.org/pdf/2503.13721), **Z Yuan**, Z Yang, Y Cai, et al. 
- ``AAAI 2025`` [Dual-level precision edges guided multi-view stereo with accurate planarization.](https://ojs.aaai.org/index.php/AAAI/article/view/32208/34363), K Chen, **Z Yuan**, T Mao, et al. 
- ``AAAI 2025`` [Mapexpert: Online hd map construction with simple and efficient sparse map element expert.](https://ojs.aaai.org/index.php/AAAI/article/download/33616/35771), D Zhang, D Chen, P Zhi, Y Chen, **Z Yuan**, et al. 
- ``AAAI 2025`` [DVP-MVS: Synergize depth-edge and visibility prior for multi-view stereo.](https://ojs.aaai.org/index.php/AAAI/article/view/33056/35211), **Z Yuan**, J Luo, F Shen, et al. 
- ``AAAI 2025`` [MSP-MVS: Multi-granularity segmentation prior guided multi-view stereo.](https://ojs.aaai.org/index.php/AAAI/article/download/33057/35212), **Z Yuan**, C Liu, F Shen, et al. 
- ``Preprint`` [Light4gs: Lightweight compact 4d gaussian splatting generation via context model.](https://arxiv.org/pdf/2503.13948?), M Liu, Q Yang, H Huang, W Huang, **Z Yuan**, et al. 
- ``Preprint`` [Adaptive label correction for robust medical image segmentation with noisy labels.](https://arxiv.org/pdf/2503.12218), C Qian, K Han, J Ding, L Liu, C Lyu, **Z Yuan**, et al. 
- ``Preprint`` [Dyncim: Dynamic curriculum for imbalanced multimodal learning.](https://arxiv.org/pdf/2503.06456), C Qian, K Han, J Wang, **Z Yuan**, et al. 
- ``PR 2025`` [Nerf-based polarimetric multi-view stereo.](https://www.sciencedirect.com/science/article/pii/S0031320324007878), J Cao, **Z Yuan**, T Mao, et al. 
- ``PR 2024`` [Tsar-mvs: Textureless-aware segmentation and correlative refinement guided multi-view stereo.](https://arxiv.org/pdf/2308.09990), **Z Yuan**, J Cao, Z Wang, et al. 
- ``AAAI 2024`` [Sd-mvs: Segmentation-driven deformation multi-view stereo with spherical refinement and em optimization.](https://ojs.aaai.org/index.php/AAAI/article/view/28512/28998), **Z Yuan**, J Cao, Z Li, et al.

# ğŸ† Awards and Service
- *2024.12* Lenovo Enterprise Scholarship (Top 3%)
- *2025.10* ICT National Scholarships (Top 5%)
- Conference Reviewers: NeurIPS, ICML, ICLR, CVPR, ICCV, ECCV, AAAI
- Journal Reviewers: IJCV, TIP, TMM, TNNLS, TCSVT, PR

<hr>
<div>
<center>
  <b>Visitor Statistics</b> <br>
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=600&t=n&d=eNPhrhf2j2RPUEw77fx7r9arT_fbosRAdIEQj-h5Bf8&co=2d79ad'></script>
</center>
</div>

<div style="height:20px;">
</div>
